{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67e9b1c7cdfc1ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6e475e3d015d0dc",
   "metadata": {},
   "source": [
    "## Creación de un simple ChatBot que interactúa con llama 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee809484ba7879e",
   "metadata": {},
   "source": [
    "### 1. Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20561cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.3.9 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.3.9)\n",
      "Requirement already satisfied: langchain-core==0.3.21 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-ollama==0.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain==0.3.9) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain==0.3.9) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain==0.3.9) (3.11.11)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain==0.3.9) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain==0.3.9) (0.1.147)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain==0.3.9) (2.1.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain==0.3.9) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain==0.3.9) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain==0.3.9) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core==0.3.21) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core==0.3.21) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core==0.3.21) (4.12.2)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-ollama==0.2.0) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.9) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.9) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.9) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.9) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.9) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.9) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.9) (1.18.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.21) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.9) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.9) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.9) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.9) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.9) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2->langchain==0.3.9) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2->langchain==0.3.9) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2->langchain==0.3.9) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2->langchain==0.3.9) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.9) (3.1.1)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.9) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.9) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.9) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.9) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain==0.3.9 langchain-core==0.3.21 langchain-ollama==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d2707c728a2d18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T19:50:56.686095Z",
     "start_time": "2024-11-21T19:50:56.060193Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf26fb11c3b0d",
   "metadata": {},
   "source": [
    "## 2. Large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab209c5e57c5d3",
   "metadata": {},
   "source": [
    "Hay diferentes modelos que se pueden usar. La mayoría necesita un \"API KEY\":\n",
    "\n",
    "ChatOpenAI, ChatAnthropic, ChatVertexAI, ChatCohere, ChatNVIDIA, ChatGroq, ChatMistralAI\n",
    "\n",
    "Existen otros modelos en la plataforma HUGGINGFACE.\n",
    "\n",
    "Para la mayoría de los modelos hay que definir una variable de entorno:\n",
    "\n",
    "------------------------------------------------------------------------------\n",
    "import getpass\n",
    "\n",
    "import os\n",
    "\n",
    "export OPENAI_API_KEY=\"...\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "Usando Ollama en local no es necesario definir variables de entorno. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T19:51:29.667040Z",
     "start_time": "2024-11-21T19:51:29.646351Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_llm = 'llama3.2'\n",
    "llm = ChatOllama(model=local_llm, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca09143b4658d2",
   "metadata": {},
   "source": [
    "Algunos de los parámetros más importantes que se deben/pueden especificar:\n",
    "\n",
    "-**model**: el nombre del modelo específico que se quiere usar (por ejemplo, para ChatOpenAI puede ser \"gpt-3.5-turbo\" o \"gpt-4\")\n",
    "\n",
    "**temperature**: controla la aleatoriedad de la respuesta (y la capacidad \"generativa\"), el valor mínimo es 0 (muy baja aleatoriedad y creatividad).\n",
    "\n",
    "**timeout**: el máximo tiempo de espera para obtener la respuesta\n",
    "\n",
    "**max_tokens**: es un número que limita el valor máximo de palabra y puntuación en la respuesta.\n",
    "\n",
    "**api_key**: el api key del usuario\n",
    "\n",
    "No todos los modelos admiten los mismos parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc276cd940e2d89e",
   "metadata": {},
   "source": [
    "Métodos clave del modelo de chat:\n",
    "\n",
    "-**invoke**: El método principal para interactuar con un modelo de chat. Acepta una lista de mensajes como entrada y devuelve una lista de mensajes como salida.\n",
    "\n",
    "-**stream**: Un método que permite transmitir la salida de un modelo de chat a medida que se genera.\n",
    "\n",
    "-**batch**: Un método que permite agrupar varias solicitudes a un modelo de chat para su procesamiento eficiente.\n",
    "\n",
    "-**bind_tools**: Un método que permite vincular una herramienta a un modelo de chat para su uso en el contexto de ejecución del modelo.\n",
    "\n",
    "-**with_structured_output**: Un envoltorio alrededor del método invoke para modelos que admiten natively salida estructurada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cb68249f90d9421",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T19:56:11.458246Z",
     "start_time": "2024-11-21T19:56:06.271833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hola! La traducción al inglés de \"El curso es interesante\" sería:\\n\\n\"The course is interesting.\"\\n\\nO también podríamos decir:\\n\\n\"The course is engaging\" o\\n\"The course is fascinating\"\\n\\nDependiendo del contexto y la intención que se quiera transmitir.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-01-15T21:20:20.659407Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5613647236, 'load_duration': 26347311, 'prompt_eval_count': 40, 'prompt_eval_duration': 695000000, 'eval_count': 61, 'eval_duration': 4890000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-6ba15b98-659f-4df7-9d20-fe7909448fbd-0', usage_metadata={'input_tokens': 40, 'output_tokens': 61, 'total_tokens': 101})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages= \"Hola, traduce al inglés: 'El curso es interesante'\"\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b84ad75b25a89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6c4a70a22679655",
   "metadata": {},
   "source": [
    "### 3. Mensajes\n",
    "\n",
    "Un mensaje suele consistir en las siguientes piezas de información:\n",
    "\n",
    "-**Rol**: El rol del mensaje (por ejemplo, \"usuario\", \"asistente\").\n",
    "\n",
    "-**Contenido**: El contenido del mensaje (por ejemplo, texto, datos multimodales).\n",
    "\n",
    "-**Metadatos adicionales**: id, nombre, uso de tokens y otros metadatos específicos del modelo.\n",
    "\n",
    "Rol\n",
    "Los roles se utilizan para distinguir entre diferentes tipos de mensajes en una conversación y ayudar al modelo de chat a entender cómo responder a una secuencia determinada de mensajes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2e7721f8ee3daea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T19:59:40.630742Z",
     "start_time": "2024-11-21T19:59:40.627809Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from english to spanish\"),\n",
    "    HumanMessage(content=\"Hello everybody, how are you doing?\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b40ea6efd755e7",
   "metadata": {},
   "source": [
    "Se puede usar un \"parser\" para escribir la respuesta sin metedatos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e856039318cc1f4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:01:04.397393Z",
     "start_time": "2024-11-21T20:01:03.205263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola a todos, ¿cómo está usted?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "result = llm.invoke(messages)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8226eaac7f614b7",
   "metadata": {},
   "source": [
    "Se pueden juntar el modelo y el parser en una \"chain\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e77666759cdde8df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:01:25.164581Z",
     "start_time": "2024-11-21T20:01:24.358762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola a todos, ¿cómo está usted?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = llm | parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8417bbf15244e2f6",
   "metadata": {},
   "source": [
    "**Temperature**\n",
    "\n",
    "Repetimos 5 veces la pregunta \"Escribe un color cualquiera\" y analizamos las respuestas para diferentes valores del parámetro \"temperature\". Para un valor de temperature=0:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88a691e0fd7d8349",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:13:24.537977Z",
     "start_time": "2024-11-21T20:13:18.876665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El color que te voy a describir es... **azul marino**.\n",
      "\n",
      "Imagina un cielo claro y soleado en una playa tropical. El azul marino es como el color del agua que se refleja en la superficie de la tierra, con un tono más oscuro y profundo que evoca sentimientos de calma y serenidad. Es un color que puede variar desde tonos suaves y pastel hasta más oscuros y dramáticos, dependiendo del ángulo de la luz y el entorno en el que se encuentra.\n",
      "\n",
      "¿Te gusta este color?\n",
      "El color que te voy a describir es... **azul marino**.\n",
      "\n",
      "Imagina un cielo claro y soleado en una playa tropical. El azul marino es como el color del agua que se refleja en la superficie de la tierra, con un tono más oscuro y profundo que evoca sentimientos de calma y serenidad. Es un color que puede variar desde tonos suaves y pastel hasta más oscuros y dramáticos, dependiendo del ángulo de la luz y el entorno en el que se encuentra.\n",
      "\n",
      "¿Te gusta este color?\n",
      "El color que te voy a describir es... **azul marino**.\n",
      "\n",
      "Imagina un cielo claro y soleado en una playa tropical. El azul marino es como el color del agua que se refleja en la superficie de la tierra, con un tono más oscuro y profundo que evoca sentimientos de calma y serenidad. Es un color que puede variar desde tonos suaves y pastel hasta más oscuros y dramáticos, dependiendo del ángulo de la luz y el entorno en el que se encuentra.\n",
      "\n",
      "¿Te gusta este color?\n",
      "El color que te voy a describir es... **azul marino**.\n",
      "\n",
      "Imagina un cielo claro y soleado en una playa tropical. El azul marino es como el color del agua que se refleja en la superficie de la tierra, con un tono más oscuro y profundo que evoca sentimientos de calma y serenidad. Es un color que puede variar desde tonos suaves y pastel hasta más oscuros y dramáticos, dependiendo del ángulo de la luz y el entorno en el que se encuentra.\n",
      "\n",
      "¿Te gusta este color?\n",
      "El color que te voy a describir es... **azul marino**.\n",
      "\n",
      "Imagina un cielo claro y soleado en una playa tropical. El azul marino es como el color del agua que se refleja en la superficie de la tierra, con un tono más oscuro y profundo que evoca sentimientos de calma y serenidad. Es un color que puede variar desde tonos suaves y pastel hasta más oscuros y dramáticos, dependiendo del ángulo de la luz y el entorno en el que se encuentra.\n",
      "\n",
      "¿Te gusta este color?\n"
     ]
    }
   ],
   "source": [
    "messages = 'Hola, escribe un color cualquiera'\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "chain = llm | parser\n",
    "for i in range(5):\n",
    "    print(chain.invoke(messages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e6899eab795f",
   "metadata": {},
   "source": [
    "Para el valor temperature=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e78d574fa7e94211",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:14:35.435018Z",
     "start_time": "2024-11-21T20:14:26.332131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El color que elegiré... sería... **naranja**\n",
      "¡Hola! El color que te voy a escribir es: **turquesa**.\n",
      "**Rojo**\n",
      "\n",
      "¡Espero que te guste! El rojo es un color vibrante y emocionante que puede evocar sentimientos de pasión, energía y entusiasmo. ¿Quieres saber más sobre este fascinante color?\n",
      "¡Hola!\n",
      "\n",
      "El color que he pensado es... **Azul Marino**.\n",
      "\n",
      "¿Te gusta?\n",
      "El color que te propongo es... **azul cobalto**. Un color vibrante y elegante que evoca sentimientos de confianza y sofisticación. ¿Te gusta?\n"
     ]
    }
   ],
   "source": [
    "messages = 'Hola, escribe un color cualquiera'\n",
    "llm = ChatOllama(model=local_llm, temperature=1)\n",
    "chain = llm | parser\n",
    "for i in range(5):\n",
    "    print(chain.invoke(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648613d13e4eb6db",
   "metadata": {},
   "source": [
    "**top-k y top-P**\n",
    "\n",
    "Al igual que la temperatura, los parámetros top-K y top-P también se utilizan para controlar la diversidad de la salida del modelo.\n",
    "\n",
    "Top-K es un número entero positivo que define la cantidad de tokens más probables de los cuales seleccionar el token de salida. Un top-K de 1 selecciona un solo token.\n",
    "\n",
    "Top-P define el umbral de probabilidad que, una vez excedido de forma acumulativa, los tokens dejan de ser seleccionados como candidatos. Un top-P de 0 es equivalente típicamente al top K con k=1, y un top-P de 1 selecciona típicamente todos los tokens en el vocabulario del modelo.\n",
    "\n",
    "Ejecute este ejemplo varias veces, cambie la configuración y observe el cambio en la salida.\n",
    " \n",
    "- top k =64  top_p = 0.95\n",
    "\n",
    "- top k=1 top_p =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "520ba039b748d467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:26:17.419229Z",
     "start_time": "2024-11-21T20:25:42.739272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Lost in the Code of Dreams\"\n",
      "\n",
      "As he sat at his desk, staring at lines of code that refused to compile, Jack's thoughts strayed to a world beyond pixels and pixels. He typed in one last line, and the room began to shimmer. The computer screen dissolved into a canvas of starry night sky. With a thrill, Jack stepped out of the void and onto a mountain path where binary symbols bloomed like flowers. He followed their trail, weaving through forests of circuitry, until he reached a glade where an ancient tree whispered secrets of code and magic to those who listened.\n"
     ]
    }
   ],
   "source": [
    "messages = \"You are a creative writer. Write a short story about a programmer who goes on an adventure. Less than 100 words. Give it an amazing title.\"\n",
    "llm = ChatOllama(model=local_llm, temperature=1, top_k=64,\n",
    "        top_p=0.95)\n",
    "chain = llm | parser\n",
    "print(chain.invoke(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c29fb6084bd0966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:27:29.516198Z",
     "start_time": "2024-11-21T20:26:39.268207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\"The Code of the Ancients\"**\n",
      "\n",
      "As a renowned programmer, Max had spent his life solving digital puzzles. But when he stumbled upon an ancient artifact in his code, he was drawn into a world beyond screens. He embarked on a perilous quest to unravel the secrets of the \"Eternal Loop.\" With each step, the code grew more complex, and the stakes higher. As Max navigated treacherous landscapes of 1s and 0s, he discovered that the true power of coding lay not in solving problems, but in unlocking the hidden potential within himself. The loop had become a doorway to his own destiny.\n"
     ]
    }
   ],
   "source": [
    "messages = \"You are a creative writer. Write a short story about a programmer who goes on an adventure. Less than 100 words. Give it an amazing title.\"\n",
    "llm = ChatOllama(model=local_llm, temperature=1, top_k=1,\n",
    "        top_p=0)\n",
    "chain = llm | parser\n",
    "print(chain.invoke(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7d135ca719b6130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:30:45.480944Z",
     "start_time": "2024-11-21T20:29:54.503023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\"The Code of the Ancients\"**\n",
      "\n",
      "As a renowned programmer, Max had spent his life solving digital puzzles. But when he stumbled upon an ancient artifact in his code, he was drawn into a world beyond screens. He embarked on a perilous quest to unravel the secrets of the \"Eternal Loop.\" With each step, the code grew more complex, and the stakes higher. As Max navigated treacherous landscapes of 1s and 0s, he discovered that the true power of coding lay not in solving problems, but in unlocking the hidden potential within himself. The loop had become a doorway to his own destiny.\n"
     ]
    }
   ],
   "source": [
    "messages = \"You are a creative writer. Write a short story about a programmer who goes on an adventure. Less than 100 words. Give it an amazing title.\"\n",
    "llm = ChatOllama(model=local_llm, temperature=1, top_k=5,\n",
    "        top_p=0)\n",
    "chain = llm | parser\n",
    "print(chain.invoke(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feecf481711e508",
   "metadata": {},
   "source": [
    "## 4. Prompt templates\n",
    "\n",
    "Si se quieren hacer preguntas siempre del mismo tipo, dando las mismas instrucciones al sistema, se pueden definir \"prompt templates\", que pueden contener variables, definidas entre {}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8328963faa7b4f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:05:13.245816Z",
     "start_time": "2024-11-21T20:05:13.242267Z"
    }
   },
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language}, write only the translated word:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7997d3031ff3b160",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:53:20.103655Z",
     "start_time": "2024-11-21T20:53:20.100003Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e2e513973b3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:53:21.633014Z",
     "start_time": "2024-11-21T20:53:21.618308Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "result = prompt_template.invoke({\"language\": \"Spanish\", \"text\": \"Hello!\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3782042470db8cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:53:25.711320Z",
     "start_time": "2024-11-21T20:53:25.706511Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fb8794710829214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:53:32.121166Z",
     "start_time": "2024-11-21T20:53:27.501663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"language\": \"spanish\", \"text\": \"Hello\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84ee819fef100606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:53:47.718716Z",
     "start_time": "2024-11-21T20:53:45.841225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wear easy-to-remove shoes for speedy security checks at airports.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a flight attendant\"),\n",
    "    (\"user\", \"Tell me a tip about {topic}. Less than 10 words\")\n",
    "])\n",
    "\n",
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\"topic\": \"security\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2854038925b328",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
